# IBM-Machine-Learning

<a href='https://github.com/AmrEidAbdelrahman/IBM-Machine-Learning#regression'>Regression</a><br>
<a href='https://github.com/AmrEidAbdelrahman/IBM-Machine-Learning#classification'>Classification</a>


# AI vs ML vs DL
- AI : is a general field with a broad scope include
  - computer vision
  - language processing
  - creativity
  - etc...
- ML : is a branch of AI that cover the statistical part include
  - Regression
  - Classification
  - Neural Network
  - etc...
- DL : is a very special field of ML and it is a revolution of ML.




# Major Machine Learning techniques
- Regression/Estimation
  predicting continuous values
- Classification
  predicting the class/category of the case
- Clustering
  find the structure of the data and group similiar together
- Association
  associate frequent co-occurring items
- Anomaly Detection
  discover ubnormal and unusual cases
- Sequance mining
  predict next step
- Dimension Reduction
  reducting the size of the data (PCA)
- Recommendation System
  recommending items
  
  
# More about scikit-learn
- free software machine learning library
- have most of regression, classification abd clustering algorithms
- work with numpy and scipy
- have a great organized documentation
- easy to implemnet
- all phases of machine learning process are implemented including
  - Data preprocessing
  - Feature extraction
  - Feature Selection
  - Train/Test split
  - Algorithm setup
  - Model fitting
  - Prediction
  - Evaluation
  - Model export
  
# Supervised vs Unsupervised Learning
- supervised : we teach the model, then with that knowledge, it can predict unknown or future instance. (Classification and Regression)
  - Labeled Data
  - Has more evaluation models that unsupervised learning
  - controlled environment
- unsupervised : the model work on its own to discover information. (Clustering, Dimension Reduction and Density estimation)
  - Unlabeled Data
  - have fewer evaluation models that supervised learning
  - less controlled environment

## what is Classification ?
Classification is the proess of predicting discrete class label.

## what is Regression ?
Regression is the process of predicting continupus values.

## What is Clustering ?
Clustering is consedered to be the most popular unsupervised machine learning techniques use for grouping data point or object that is some how similar.
used in :
  - Discovering structure
  - Summerization
  - Anomaly detection


# Regression
regression have two main variable X:Independent Variable and Y:Dependet Variable
X:Idependent Variable also known as Explantory variable is the causes of the target variable.
Y:Dependent Variable is the target variable, we study and try to predict.
in regression, The Y:Dependent variable have to be continuouse not discrete/categorical.
#### Type of Regression
- Simple Linear Regression
  - simple Linear regression
  - Simple Non-linear regression
- Multiple Linear Regression
  - Multiple Linear regression
  - Multiple Non-linear regression
- Polynomial Regression
#### Regression algorithms
- Ordinal Regression
- Poisson Regression
- Fast forest quantile regression
- linear, polynomial, Lasso, Stepwise, Ridge regression
- Bayesian linear regression
- Newural network regression
- Decision forest regression
- Boosted decision tree regression
- KNN (K-nearest neighbors)

## Simple Linear Regression :
- this is how simple linear regression works
  <br><img src='img/Linear Regression model representation.jpg'><br>
  s.t seta0 and seta1 are the cooffecient of the fit line
 
- Find the best fit <br>
  <img src='img/Linear Regression find the best fit.jpg'><br>
  
- pros:
  - very fast.
  - no need to parameters tunning
  - Easy to understand, and highly interpretable.
  
- Accuracy:
  1. first approach: train and test on the same data
  <br><img src='img/Linear Regression Accuracy.jpg'><br>
    ```this give us High training accuracy( accuracy of the data we train the model on ) and low out of sample accuracy ( accuracy of other data the model didn't see before )```
    overfit model 
    
  2. second approach: train/test split 
    more accurate evaluation on out of sample accuracy 
    
  3. k-folds cross validation<br>
    <img src='img/k-folds cross validation.jpg'><br>
    
- Error of a model: The difference between the data points and the trend line generated by the algorithm  
  There are many ways to calculate the error of a model...
  <br>
  <img src='img/model error.jpg'><br>
  1. MEAN ABSOLUATE ERROR(MAE)
  2. MEAN SQUARED ERROR(MSE)
  3. ROOT MEAN SQUARED ERROR(RMSE)
  4. RELATIVE ABSOLUTE ERROR(RAE)
  5. RELATIVE SQUARED ERROR(RSE)
  6. R-Squared(R^2)

## Multiple linear regression <br>
<br><img src='img/multiple linear regression.jpg'><br>
  - model error: like in simple LR we can use MSE to find the error
    <br><br><img src='img/Multiple model error.jpg'><br>
    
  - how to find the parameter seta ?
    1. using Linear algebra operations
      - Ordinal Least Squares: we need to find the minimum error with iterate the process but it take a long time with the larg data set
    2. using optmization algorithm
      - gradient descent ( this approach work with a larg data set ) <a href=''>read more</a>
    
  - important quetion in case to use [multiple] linear regression <br>
    <img src='img/Question about Multiple linear regression.jpg'><br>

## Polynomail Regression ( non-linear regression )
<br><img src='img/non linear regression.jpg'><br><br>
some curvy data can be modelated by a polynomial regression
  - a polynomial regression model can be transformed into linear regression model
  - can be fit with least squares algorithm

## Linear vs non-Linear Regression
<br><img src='img/linear vs non-linear.jpg'><br>


**===========================================================================================================================================**

# Classification

### What is classification
- is a supervise machine learning algorithm
- Categorizing (classifying) some unkown items into a discret set of categorize (classes) 

### classification algorithms:
- Decision Tree
- Naive bayes
- Linear Discriminant Analysis
- k-nearst neighbour
- Logistic regression
- Neural Networks
- Support Vector Machine (SVM)


## K-Nearst Neighbour
The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. 
The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.
<br><img src='https://miro.medium.com/max/611/1*wW8O-0xVQUFhBGexx2B6hg.png'><br>
KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics.
There are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice.
<br><br><img src='img/Knn algorithm.jpg'><br>
#### Choosing the right value for K:
To select the K that’s right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm’s ability to accurately make predictions when it’s given data it hasn’t seen before.


#### Evaluation Metrics
- Jaccard Index:
  <br><img src='img/Jaccard index evaluation.jpg'><br>
- F1 score:
  <br><img src='img/F1 score.jpg'><br>
- log loss:
  <br><img src='img/log loss evaluation.jpg'><br>

#### Advantage
- The algorithm is simple and easy to implement.
- There’s no need to build a model, tune several parameters, or make additional assumptions.
- The algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section).

#### Disadvantages
- The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.



#### references:
<a href'https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761'>KNN Explanation</a>


## Decision Tree
<br><img src='img/decision tree algorithm.jpg'><br>

#### Entropy :
The entropy in a node is the amount of information disorder calculated in each node.

#### Information Gain:
Another term worth noting is “Information Gain” which is used with splitting the data using entropy. It is calculated as the decrease in entropy after the dataset is split on an attribute:
```
Gain(T,X) = Entropy(T) — Entropy(T,X)
- T = target variable
- X = Feature to be split on
- Entropy(T,X) = The entropy calculated after the data is split on feature X
```

#### Node impurity / Impurity Criterion:
Both Scikit-learn and Spark provide information in their documentation on the formulas used for impurity criterion. For classification, they both use Gini impurity by default but offer Entropy as an alternative. For regression, both calculate variance reduction using Mean Square Error. Additionally, variance reduction can be calculated with Mean Absolute Error in Scikit-learn.
<br><img src='https://miro.medium.com/max/630/1*eES0Bh8jTB73P3ad_U2aCA.png'><br>

