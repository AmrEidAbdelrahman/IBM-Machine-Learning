# IBM-Machine-Learning

<a href='https://github.com/AmrEidAbdelrahman/IBM-Machine-Learning#regression'>Regression</a><br>
<a href='https://github.com/AmrEidAbdelrahman/IBM-Machine-Learning#classification'>Classification</a>


# AI vs ML vs DL
- AI : is a general field with a broad scope include
  - computer vision
  - language processing
  - creativity
  - etc...
- ML : is a branch of AI that cover the statistical part include
  - Regression
  - Classification
  - Neural Network
  - etc...
- DL : is a very special field of ML and it is a revolution of ML.




# Major Machine Learning techniques
- Regression/Estimation
  predicting continuous values
- Classification
  predicting the class/category of the case
- Clustering
  find the structure of the data and group similiar together
- Association
  associate frequent co-occurring items
- Anomaly Detection
  discover ubnormal and unusual cases
- Sequance mining
  predict next step
- Dimension Reduction
  reducting the size of the data (PCA)
- Recommendation System
  recommending items
  
  
# More about scikit-learn
- free software machine learning library
- have most of regression, classification abd clustering algorithms
- work with numpy and scipy
- have a great organized documentation
- easy to implemnet
- all phases of machine learning process are implemented including
  - Data preprocessing
  - Feature extraction
  - Feature Selection
  - Train/Test split
  - Algorithm setup
  - Model fitting
  - Prediction
  - Evaluation
  - Model export
  
# Supervised vs Unsupervised Learning
- supervised : we teach the model, then with that knowledge, it can predict unknown or future instance. (Classification and Regression)
  - Labeled Data
  - Has more evaluation models that unsupervised learning
  - controlled environment
- unsupervised : the model work on its own to discover information. (Clustering, Dimension Reduction and Density estimation)
  - Unlabeled Data
  - have fewer evaluation models that supervised learning
  - less controlled environment

## what is Classification ?
Classification is the proess of predicting discrete class label.

## what is Regression ?
Regression is the process of predicting continupus values.

## What is Clustering ?
Clustering is consedered to be the most popular unsupervised machine learning techniques use for grouping data point or object that is some how similar.
used in :
  - Discovering structure
  - Summerization
  - Anomaly detection


# Regression
regression have two main variable X:Independent Variable and Y:Dependet Variable
X:Idependent Variable also known as Explantory variable is the causes of the target variable.
Y:Dependent Variable is the target variable, we study and try to predict.
in regression, The Y:Dependent variable have to be continuouse not discrete/categorical.
#### Type of Regression
- Simple Linear Regression
  - simple Linear regression
  - Simple Non-linear regression
- Multiple Linear Regression
  - Multiple Linear regression
  - Multiple Non-linear regression
- Polynomial Regression
#### Regression algorithms
- Ordinal Regression
- Poisson Regression
- Fast forest quantile regression
- linear, polynomial, Lasso, Stepwise, Ridge regression
- Bayesian linear regression
- Newural network regression
- Decision forest regression
- Boosted decision tree regression
- KNN (K-nearest neighbors)

## Simple Linear Regression :
- this is how simple linear regression works
  <br><img src='img/Linear Regression model representation.jpg'><br>
  s.t seta0 and seta1 are the cooffecient of the fit line
 
- Find the best fit <br>
  <img src='img/Linear Regression find the best fit.jpg'><br>
  
- pros:
  - very fast.
  - no need to parameters tunning
  - Easy to understand, and highly interpretable.
  
- Accuracy:
  1. first approach: train and test on the same data
  <br><img src='img/Linear Regression Accuracy.jpg'><br>
    ```this give us High training accuracy( accuracy of the data we train the model on ) and low out of sample accuracy ( accuracy of other data the model didn't see before )```
    overfit model 
    
  2. second approach: train/test split 
    more accurate evaluation on out of sample accuracy 
    
  3. k-folds cross validation<br>
    <img src='img/k-folds cross validation.jpg'><br>
    
- Error of a model: The difference between the data points and the trend line generated by the algorithm  
  There are many ways to calculate the error of a model...
  <br>
  <img src='img/model error.jpg'><br>
  1. MEAN ABSOLUATE ERROR(MAE)
  2. MEAN SQUARED ERROR(MSE)
  3. ROOT MEAN SQUARED ERROR(RMSE)
  4. RELATIVE ABSOLUTE ERROR(RAE)
  5. RELATIVE SQUARED ERROR(RSE)
  6. R-Squared(R^2)

## Multiple linear regression <br>
<br><img src='img/multiple linear regression.jpg'><br>
  - model error: like in simple LR we can use MSE to find the error
    <br><br><img src='img/Multiple model error.jpg'><br>
    
  - how to find the parameter seta ?
    1. using Linear algebra operations
      - Ordinal Least Squares: we need to find the minimum error with iterate the process but it take a long time with the larg data set
    2. using optmization algorithm
      - gradient descent ( this approach work with a larg data set ) <a href=''>read more</a>
    
  - important quetion in case to use [multiple] linear regression <br>
    <img src='img/Question about Multiple linear regression.jpg'><br>

## Polynomail Regression ( non-linear regression )
<br><img src='img/non linear regression.jpg'><br><br>
some curvy data can be modelated by a polynomial regression
  - a polynomial regression model can be transformed into linear regression model
  - can be fit with least squares algorithm

## Linear vs non-Linear Regression
<br><img src='img/linear vs non-linear.jpg'><br>


**===========================================================================================================================================**

# Classification

### What is classification
- is a supervise machine learning algorithm
- Categorizing (classifying) some unkown items into a discret set of categorize (classes) 

### classification algorithms:
- Decision Tree
- Naive bayes
- Linear Discriminant Analysis
- k-nearst neighbour
- Logistic regression
- Neural Networks
- Support Vector Machine (SVM)


## K-Nearst Neighbour
The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. 
The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.
<br><img src='https://miro.medium.com/max/611/1*wW8O-0xVQUFhBGexx2B6hg.png'><br>
KNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics.
There are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice.
<br><br><img src='img/Knn algorithm.jpg'><br>
#### Choosing the right value for K:
To select the K that’s right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm’s ability to accurately make predictions when it’s given data it hasn’t seen before.


#### Evaluation Metrics
- Jaccard Index:
  <br><img src='img/Jaccard index evaluation.jpg'><br>
- F1 score:
  <br><img src='img/F1 score.jpg'><br>
- log loss:
  <br><img src='img/log loss evaluation.jpg'><br>

#### Advantage
- The algorithm is simple and easy to implement.
- There’s no need to build a model, tune several parameters, or make additional assumptions.
- The algorithm is versatile. It can be used for classification, regression, and search (as we will see in the next section).

#### Disadvantages
- The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.



#### references:
<a href'https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761'>KNN Explanation</a>


## Decision Tree
Decision trees learn how to best split the dataset into smaller and smaller subsets to predict the target value. The condition, or test, is represented as the “leaf” (node) and the possible outcomes as “branches” (edges). This splitting process continues until no further gain can be made or a preset rule is met, e.g. the maximum depth of the tree is reached.
<br><img src='img/decision tree algorithm.jpg'><br>

#### Entropy :
The entropy in a node is the amount of information disorder calculated in each node.

#### Information Gain:
Another term worth noting is “Information Gain” which is used with splitting the data using entropy. It is calculated as the decrease in entropy after the dataset is split on an attribute:
```
Gain(T,X) = Entropy(T) — Entropy(T,X)
- T = target variable
- X = Feature to be split on
- Entropy(T,X) = The entropy calculated after the data is split on feature X
```

#### Node impurity / Impurity Criterion:
Both Scikit-learn and Spark provide information in their documentation on the formulas used for impurity criterion. For classification, they both use Gini impurity by default but offer Entropy as an alternative. For regression, both calculate variance reduction using Mean Square Error. Additionally, variance reduction can be calculated with Mean Absolute Error in Scikit-learn.
<br><img src='https://miro.medium.com/max/630/1*eES0Bh8jTB73P3ad_U2aCA.png'><br>
<!--
#### advatages for ID3:
- Builds the fastest tree
- Understandable prediction rules are created from the training data

#### disadvantages ID3:
- Data may be over-fitted or over-classified, if a small sample is tested
- Only one attribute at a time is tested for making a decision
- Does not handle numeric attributes and missing values
-->

#### references:
<a href='https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3'>Decision Tree</a>


## Logistic Regression
- Logistic regression is analogous to linear regression but takes a categorical/discrete target field instead of a numeric one.
- Logistic Regression measures the probability of a case belonging to a specific class.
- Logistic Regression can be used to understand the impact of a feature on a dependent variable.
- Data is fit into linear regression model, which then be acted upon by a logistic function predicting the target categorical dependent variable.

#### When is logisitc regression suitable ?
- if your data is binary.
- if you need probalisitic resullt
- when you need a linear decision boundry
- if you need to understand the impact of each feature

<a id="ref1"></a>

## What is the difference between Linear and Logistic Regression?

While Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class of an observed data point. In order to estimate the class of a data point, we need some sort of guidance on what would be the <b>most probable class</b> for that data point. For this, we use <b>Logistic Regression</b>.

<div class="alert alert-success alertsuccess" style="margin-top: 20px">
<font size = 3><strong>Recall linear regression:</strong></font>
<br>
<br>
    As you know, <b>Linear regression</b> finds a function that relates a continuous dependent variable, <b>y</b>, to some predictors (independent variables $x_1$, $x_2$, etc.). For example, simple linear regression assumes a function of the form:
<br><br>
$$
y = \theta_0 + \theta_1  x_1 + \theta_2  x_2 + \cdots
$$
<br>
and finds the values of parameters $\theta_0, \theta_1, \theta_2$, etc, where the term $\theta_0$ is the "intercept". It can be generally shown as:
<br><br>
$$
ℎ_\theta(𝑥) = \theta^TX
$$
<p></p>

</div>

Logistic Regression is a variation of Linear Regression, used when the observed dependent variable, <b>y</b>, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.

Logistic regression fits a special s-shaped curve by taking the linear regression function and transforming the numeric estimate into a probability with the following function, which is called the sigmoid function 𝜎:

$$
ℎ\_\theta(𝑥) = \sigma({\theta^TX}) =  \frac {e^{(\theta\_0 + \theta\_1  x\_1 + \theta\_2  x\_2 +...)}}{1 + e^{(\theta\_0 + \theta\_1  x\_1 + \theta\_2  x\_2 +\cdots)}}
$$
Or:
$$
ProbabilityOfaClass\_1 =  P(Y=1|X) = \sigma({\theta^TX}) = \frac{e^{\theta^TX}}{1+e^{\theta^TX}}
$$

In this equation, ${\theta^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\sigma(\theta^TX)$ is the sigmoid or [logistic function](http://en.wikipedia.org/wiki/Logistic_function?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML0101ENSkillsNetwork20718538-2021-01-01), also called logistic curve. It is a common "S" shape (sigmoid curve).

So, briefly, Logistic Regression passes the input through the logistic/sigmoid but then treats the result as a probability:

<img
src="https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork/labs/Module%203/images/mod_ID_24_final.png" width="400" align="center">

The objective of the **Logistic Regression** algorithm, is to find the best parameters θ, for $ℎ\_\theta(𝑥)$ = $\sigma({\theta^TX})$, in such a way that the model best predicts the class of each case.


